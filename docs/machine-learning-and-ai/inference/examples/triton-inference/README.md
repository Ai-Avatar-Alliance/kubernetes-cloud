# Triton Inference

<figure><img src="../../../../.gitbook/assets/image (52).png" alt="The NVIDIA logo"><figcaption></figcaption></figure>

[NVIDIA's Triton™ Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) is a piece of Inference-serving Open Source software that helps to standardize model deployment and execution to deliver fast and scalable AI in production.

## Tutorials

<table data-card-size="large" data-view="cards"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type="content-ref"></th></tr></thead><tbody><tr><td><span data-gb-custom-inline data-tag="emoji" data-code="26a1">⚡</span> <strong>Faster Transformer GPT-J and GPT: NeoX 20B</strong></td><td></td><td></td><td><a href="triton-inference-server-fastertransformer.md">triton-inference-server-fastertransformer.md</a></td></tr></tbody></table>
