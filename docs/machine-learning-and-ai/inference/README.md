---
description: Welcome to Inference on CoreWeave
---

# Inference

**Machine learning** is one of the most popular applications of CoreWeave Cloud's state-of-the-art infrastructure. Models are easily hosted on CoreWeave, and can be sourced from a range of storage backends including [S3-compatible object storage](../../storage/object-storage.md), HTTP, or persistent [Storage Volumes](../../storage/storage/#storage-volumes).

CoreWeave Cloud's Inference engine autoscales containers based on demand in order to to swiftly fulfill user requests, then scales down according to load so as to preserve GPU resources. Allocating new resources and scaling up new containers can be as fast as 15 seconds for the 6B GPT-J model.

Allocating new resources and scaling up a container can be as fast as fifteen seconds for the 6B GPT-J model. This quick autoscale allows for a significantly more responsive service than that of other Cloud providers.

## Learn more

<table data-card-size="large" data-view="cards"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-target data-type="content-ref"></th></tr></thead><tbody><tr><td><span data-gb-custom-inline data-tag="emoji" data-code="2b50">‚≠ê</span> <strong>Get Started with Inference</strong></td><td>Learn more about the Inference engine and how to use it on CoreWeave Cloud.</td><td></td><td><a href="online-inference.md">online-inference.md</a></td></tr><tr><td><span data-gb-custom-inline data-tag="emoji" data-code="1f446">üëÜ</span> <strong>One-Click Models</strong></td><td>View Inference models that can be deployed at the click of a button.</td><td></td><td><a href="examples/models/">models</a></td></tr><tr><td><span data-gb-custom-inline data-tag="emoji" data-code="1f4d6">üìñ</span> <strong>Inference Examples</strong></td><td>Check out some additional examples of Inference projects on CoreWeave Cloud.</td><td></td><td><a href="examples/">examples</a></td></tr></tbody></table>
