# Triton Inference

<figure><img src="../../.gitbook/assets/image (52) (3).png" alt="The NVIDIA logo"><figcaption></figcaption></figure>

[NVIDIA's Tritonâ„¢ Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) is a piece of Inference-serving Open Source software that helps to standardize model deployment and execution to deliver fast and scalable AI in production.

## How-to guides and tutorials

For examples of Triton Inference projects on CoreWeave Cloud, see [Triton Inference Guides](../how-to-guides-and-tutorials/examples/triton-inference-guides/).
