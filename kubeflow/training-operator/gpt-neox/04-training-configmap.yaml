apiVersion: v1
kind: ConfigMap
metadata:
  name: neox-training
data:
  neox.yml: |
    {
      # Tokenizer /  checkpoint settings - you will need to change these to the location you have them saved in
      "vocab-file": "/mnt/checkpoints/20B_checkpoints/20B_tokenizer.json",
      "save": "/mnt/checkpoints/20B_finetune_checkpoints",
      "load": "/mnt/checkpoints/20B_checkpoints",
      
      # If finetuning, edit the following to the location of your finetuning dataset:
      "data-path": "/mnt/data/datasets/hackernews/hackernews_text_document",
      
      # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages
      # across the node boundaries )
      "pipe-parallel-size": 4,
      "model-parallel-size": 2,
      
      # model settings
      "num-layers": 44,
      "hidden-size": 6144,
      "num-attention-heads": 64,
      "seq-length": 2048,
      "max-position-embeddings": 2048,
      "norm": "layernorm",
      "pos-emb": "rotary",
      "rotary_pct": 0.25,
      "no-weight-tying": true,
      "gpt_j_residual": true,
      "output_layer_parallelism": "column",
      "scaled-upper-triang-masked-softmax-fusion": true,
      "bias-gelu-fusion": true,
      
      # init methods
      "init_method": "small_init",
      "output_layer_init_method": "wang_init",
      
      # optimizer settings
      "optimizer": {
        "type": "Adam",
        "params": {
          "lr": 0.97e-4,
          "betas": [ 0.9, 0.95 ],
          "eps": 1.0e-8,
        }
      },
      
      "min_lr": 0.97e-5,
      
      # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training
      "zero_optimization": {
        "stage": 1,
        "allgather_partitions": True,
        "allgather_bucket_size": 1260000000,
        "overlap_comm": True,
        "reduce_scatter": True,
        "reduce_bucket_size": 1260000000,
        "contiguous_gradients": True,
      },
      
      # batch / data settings (assuming 16 GPUs)
      "train_micro_batch_size_per_gpu": 8,
      "gradient_accumulation_steps": 96,  # 12,288 samples before stepping the optimizer (16 * 8 * 96)
      "data-impl": "mmap",
      "split": "995,4,1",
      
      # activation checkpointing
      "checkpoint-activations": true,
      "checkpoint-num-layers": 1,
      "partition-activations": false,
      "synchronize-each-layer": true,
      
      # regularization
      "gradient_clipping": 1.0,
      "weight-decay": 0.01,
      "hidden-dropout": 0,
      "attention-dropout": 0,
      
      # precision settings
      "fp16": {
        "fp16": true,
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 12,
        "hysteresis": 2,
        "min_loss_scale": 1
      },
      
      # misc. training settings
      "train-iters": 150_000,
      "lr-decay-iters": 150_000,
      
      "distributed-backend": "nccl",
      "lr-decay-style": "cosine",
      "warmup": 0.01,
      "checkpoint-factor": 500, # this variable previously called `save-interval`
      "eval-interval": 1000,
      "eval-iters": 10,
      
      # logging
      "log-interval": 2,
      "steps_per_print": 2,
      "wall_clock_breakdown": false,
      
      ### NEW DATA: ####
      "tokenizer_type": "HFTokenizer",
      "tensorboard-dir": "/mnt/data/tensorboard",
      "log-dir": "/mnt/data/logs",
      
      # MPI Operator
      "hostfile": "/etc/mpi/hostfile",
      "deepspeed_mpi": false,
      
      #  "wandb_team": "coreweave",
      "wandb_project": "finetune-gpt-neox",
      "wandb_group": "A100-NVLINK-2N"
    
    }